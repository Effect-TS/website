---
title: Getting Started
description: Learn how to use Effect's AI integration packages to define LLM interactions
sidebar:
  order: 1
---

import { Aside, Steps, Tabs, TabItem } from "@astrojs/starlight/components"

In this getting started guide, we will demonstrate how to generate a simple text completion using an LLM provider (OpenAi) using the Effect AI integration packages.

We’ll walk through:
- Writing provider-agnostic logic to interact with an LLM
- Declaring the specific LLM model to use for the interaction
- Using a provider integration to make the program executable

## Installation

First, we will need to install the base `@effect/ai` package to gain access to the core AI abstractions. In addition, we will need to install at least one provider integration package (in this case `@effect/ai-openai`):

<Tabs syncKey="package-manager">

<TabItem label="npm" icon="seti:npm">

```sh showLineNumbers=false
# Install the base package for the core abstractions (always required)
npm install @effect/ai

# Install one (or more) provider integrations
npm install @effect/ai-openai

# Also add the core Effect package (if not already installed)
npm install effect
```

</TabItem>

<TabItem label="pnpm" icon="pnpm">

```sh showLineNumbers=false
# Install the base package for the core abstractions (always required)
pnpm add @effect/ai

# Install one (or more) provider integrations
pnpm add @effect/ai-openai

# Also add the core Effect package (if not already installed)
pnpm add effect
```

</TabItem>

<TabItem label="Yarn" icon="seti:yarn">

```sh showLineNumbers=false
# Install the base package for the core abstractions (always required)
yarn add @effect/ai

# Install one (or more) provider integrations
yarn add @effect/ai-openai

# Also add the core Effect package (if not already installed)
yarn add effect
```

</TabItem>

<TabItem label="Bun" icon="bun">

```sh showLineNumbers=false
# Install the base package for the core abstractions (always required)
bun add @effect/ai

# Install one (or more) provider integrations
bun add @effect/ai-openai

# Also add the core Effect package (if not already installed)
bun add effect
```

</TabItem>

</Tabs>

## Define an LLM Interaction

First let's define a simple interaction with an LLM:

```ts twoslash
import { Completions } from "@effect/ai"
import { Console, Effect } from "effect"

// Using `Completions` will add it to your program's requirements
//
//          ┌─── Effect<void, AiError, Completions>
//          ▼
const generateDadJoke = Effect.gen(function*() {
  // Extract the `Completions` service from the Effect environment
  const completions = yield* Completions.Completions
  // Use the `Completions` service to generate some text
  return yield* completions.create("Generate a hilarious dad joke").pipe(
    // Log the response text to the console
    Effect.andThen((response) => Console.log(response.text))
  )
})
```

The above program uses the `Completions` service from the base `@effect/ai` package to describe using an LLM to create a text completion (in this case generating a hilarious dad joke). The result of the text generation is then logged to the console.

<Aside type="note" title="Declarative LLM Interactions">
  Notice that the above code does not know or care which LLM provider (OpenAi, Anthropic, etc.) will be used. Instead, we focus on _what_ we want to accomplish (i.e. our business logic), not _how_ to accomplish it.
</Aside>

## Select an LLM Model

Next, we need to select which provider we want to use to satisfy the `Completions` requirement of our `generateDadJoke` program:

```ts twoslash {1,17,25,27} collapse={5-10}
import { OpenAiCompletions } from "@effect/ai-openai"
import { Completions } from "@effect/ai"
import { Console, Effect } from "effect"

const generateDadJoke = Effect.gen(function*() {
  const completions = yield* Completions.Completions
  return yield* completions.create("Generate a hilarious dad joke").pipe(
    Effect.andThen((response) => Console.log(response.text))
  )
})

// Create an `AiModel` which provides a concrete implementation of 
// `Completions` and requires an `OpenAiClient`
//
//      ┌─── AiModel<Completions, OpenAiClient>
//      ▼
const Gpt4o = OpenAiCompletions.model("gpt-4o")

// Building an `AiModel` adds its requirements to the program
//
//     ┌─── Effect<void, AiError, OpenAiClient>
//     ▼
const main = Effect.gen(function*() {
  // Build the `AiModel` into a `Provider`
  const gpt4o = yield* Gpt4o
  // Provide the implementation of `Completions` to `generateDadJoke`
  yield* gpt4o.provide(generateDadJoke)
})
```

Before we can finish satisfying requirements and make our program runnable, it is important that we understand the purpose of the `AiModel` data type.

## Understanding `AiModel`

The `AiModel` data type represents a **provider-specific implementation** of one or more services, such as `Completions` or `Embeddings`. It is the primary way that you can plug a real LLM (like OpenAI’s GPT-4o) into your program.

```ts showLineNumbers=false
export interface AiModel<Provides, Requires> {}
```

An `AiModel` has two generic type parameters:

- **Provides** - the services this model will provide when built
- **Requires** - the services this model will require to be built

This allows Effect to track which services should be added to the requirements of the program that builds the `AiModel`, as well as which services the built `AiModel` can provide.

### Creating an `AiModel`

To create an `AiModel`, you can use the model-specific factory from one of Effect's provider integration packages. 

For example:

```ts showLineNumbers=false
import { OpenAiCompletions } from "@effect/ai-openai"

//      ┌─── AiModel<Completions, OpenAiClient>
//      ▼
const Gpt4o = OpenAiCompletions.model("gpt-4o")
```

This creates an `AiModel` that:

- **Provides** an OpenAi-specific implementation of the `Completions` service using `"gpt-4o"`
- **Requires** an `OpenAiClient` to be built

### Building an `AiModel`

When you build an `AiModel`, you get back a `Provider<Provides>`:

```ts showLineNumbers=false
Effect.gen(function*() {
  //      ┌─── Provider<Completions>
  //      ▼
  const gpt4o = yield* Gpt4o
})
```

A `Provider` has a single `.provide(...)` method: 

```ts showLineNumbers=false
gpt4o.provide(myAiProgram)
```

The `.provide(...)` method injects the services built by the `AiModel` into an Effect program, thereby removing those services from that program's requirements.

### Benefits of `AiModel`

There are several benefits to this approach:

**Reusability**

You can call `.provide(...)` with the same built model as many times as you like.

For example, we can re-use the same built model to provide `Completions` to multiple calls to `generateDadJoke`:

```ts twoslash {16-18} collapse={5-10}
import { OpenAiCompletions } from "@effect/ai-openai"
import { Completions } from "@effect/ai"
import { Console, Effect } from "effect"

const generateDadJoke = Effect.gen(function*() {
  const completions = yield* Completions.Completions
  return yield* completions.create("Generate a hilarious dad joke").pipe(
    Effect.andThen((response) => Console.log(response.text))
  )
})

const Gpt4o = OpenAiCompletions.model("gpt-4o")

const main = Effect.gen(function*() {
  const gpt = yield* Gpt4o
  yield* gpt.provide(generateDadJoke)
  yield* gpt.provide(generateDadJoke)
  yield* gpt.provide(generateDadJoke)
})
```

**Flexibility**

If we know that one model or provider performs better at a given task than another, we can freely mix and match models and providers together. 

For example, if we know Anthropic's Claude generates some really great dad jokes, we can mix it into our existing program with just a few lines of code:

```ts twoslash {1,14,20,23} collapse={6-11}
import { AnthropicCompletions } from "@effect/ai-anthropic"
import { OpenAiCompletions } from "@effect/ai-openai"
import { Completions } from "@effect/ai"
import { Console, Effect } from "effect"

const generateDadJoke = Effect.gen(function*() {
  const completions = yield* Completions.Completions
  return yield* completions.create("Generate a hilarious dad joke").pipe(
    Effect.andThen((response) => Console.log(response.text))
  )
})

const Gpt4o = OpenAiCompletions.model("gpt-4o")
const Claude37 = AnthropicCompletions.model("claude-3-7-sonnet-latest")

//      ┌─── Effect<void, AiError, AnthropicClient | OpenAiClient>
//      ▼
const main = Effect.gen(function*() {
  const gpt = yield* Gpt4o
  const claude = yield* Claude37
  yield* gpt.provide(generateDadJoke)
  yield* gpt.provide(generateDadJoke)
  yield* claude.provide(generateDadJoke)
})
```

Because Effect performs type-level dpeendency tracking, we can see that an `AnthropicClient` must now also be provided to make our program runnable.

**Abstractability**

Because there is separation between _building_ an `AiModel` and _providing_ its services, we can very nicely support the service constructor pattern.

For example, in the code below the `main` program is only dependent upon the `DadJokes` service. All AI requirements are abstracted away into `Layer` composition.

```ts twoslash collapse={14-19}
import { AnthropicCompletions } from "@effect/ai-anthropic"
import { OpenAiCompletions } from "@effect/ai-openai"
import { Completions } from "@effect/ai"
import { Console, Effect } from "effect"

const Gpt4o = OpenAiCompletions.model("gpt-4o")
const Claude37 = AnthropicCompletions.model("claude-3-7-sonnet-latest")

class DadJokes extends Effect.Service<DadJokes>()("app/DadJokes", {
  effect: Effect.gen(function*() {
    const gpt = yield* Gpt4o
    const claude = yield* Claude37

    const generateDadJoke = Effect.gen(function*() {
      const completions = yield* Completions.Completions
      return yield* completions.create("Generate a hilarious dad joke").pipe(
        Effect.andThen((response) => Console.log(response.text))
      )
    })

    return {
      generateDadJoke: gpt.provide(generateDadJoke),
      generateReallyGroanInducingDadJoke: claude.provide(generateDadJoke)
    }
  })
}) {}

// Programs which utilize the `DadJokes` service have no knowledge of 
// any AI requirements
//
//     ┌─── Effect<void, AiError, DadJokes>
//     ▼
const main = Effect.gen(function*() {
  const dadJokes = yield* DadJokes
  yield* dadJokes.generateDadJoke
  yield* dadJokes.generateReallyGroanInducingDadJoke
})

// The AI requirements are abstracted away into `Layer` composition
//
//         ┌─── Layer<DadJokes, never, AnthropicClient | OpenAiClient>
//         ▼
DadJokes.Default
```

## Create a Provider Client

To make our code executable, we must finish satisfying our program's requirements. 

Let's take another look at our program from earlier:

```ts twoslash "OpenAiClient" collapse={5-12}
import { OpenAiCompletions } from "@effect/ai-openai"
import { Completions } from "@effect/ai"
import { Console, Effect } from "effect"

const generateDadJoke = Effect.gen(function*() {
  const completions = yield* Completions.Completions
  return yield* completions.create("Generate a hilarious dad joke").pipe(
    Effect.andThen((response) => Console.log(response.text))
  )
})

const Gpt4o = OpenAiCompletions.model("gpt-4o")

//     ┌─── Effect<void, AiError, OpenAiClient>
//     ▼
const main = Effect.gen(function*() {
  const gpt4o = yield* Gpt4o
  yield* gpt4o.provide(generateDadJoke)
})
```

We can see that our `main` program still requires us to provide an `OpenAiClient`. 

Each of our provider integration packages exports a client module that can be used to construct a client for that provider.

For example:

```ts twoslash /{ (OpenAiClient),/ {24-26} collapse={5-17}
import { OpenAiClient, OpenAiCompletions } from "@effect/ai-openai"
import { Completions } from "@effect/ai"
import { Config, Console, Effect } from "effect"

const generateDadJoke = Effect.gen(function*() {
  const completions = yield* Completions.Completions
  return yield* completions.create("Generate a hilarious dad joke").pipe(
    Effect.andThen((response) => Console.log(response.text))
  )
})

const Gpt4o = OpenAiCompletions.model("gpt-4o")

const main = Effect.gen(function*() {
  const gpt4o = yield* Gpt4o
  yield* gpt4o.provide(generateDadJoke)
})

// Create a `Layer` which produces an `OpenAiClient` and requires
// an `HttpClient`
//
//      ┌─── Layer<OpenAiClient, never, HttpClient>
//      ▼
const OpenAi = OpenAiClient.layerConfig({
  apiKey: Config.redacted("OPENAI_API_KEY")
})
```

In the code above, we use the `layerConfig` constructor from the `OpenAiClient` module to create a `Layer` which will produce an `OpenAiClient`. The `layerConfig` constructor allows us to read in configuration variables using Effect's [configuration system](/docs/configuration/).

The provider clients also have a dependency on an `HttpClient` implementation to avoid any platform dependencies. This way, you can provide whichever `HttpClient` implementation is most appropriate for the platform your code is running upon.

For example, if we know we are going to run this code in NodeJS, we can utilize the `NodeHttpClient` module from `@effect/platform-node` to provide an `HttpClient` implementation:

```ts twoslash /{ (NodeHttpClient) }|, (HttpClient)>|, (never)>/ {34} collapse={6-18} 
import { OpenAiClient, OpenAiCompletions } from "@effect/ai-openai"
import { Completions } from "@effect/ai"
import { NodeHttpClient } from "@effect/platform-node"
import { Config, Console, Effect, Layer } from "effect"

const generateDadJoke = Effect.gen(function*() {
  const completions = yield* Completions.Completions
  return yield* completions.create("Generate a hilarious dad joke").pipe(
    Effect.andThen((response) => Console.log(response.text))
  )
})

const Gpt4o = OpenAiCompletions.model("gpt-4o")

const main = Effect.gen(function*() {
  const gpt4o = yield* Gpt4o
  yield* gpt4o.provide(generateDadJoke)
})

// Create a `Layer` which produces an `OpenAiClient` and requires
// an `HttpClient`
//
//      ┌─── Layer<OpenAiClient, never, HttpClient>
//      ▼
const OpenAi = OpenAiClient.layerConfig({
  apiKey: Config.redacted("OPENAI_API_KEY")
})

// Provide a platform-specific implementation of `HttpClient` to our 
// OpenAi layer
//
//        ┌─── Layer<OpenAiClient, never, never>
//        ▼
const OpenAiWithHttp = Layer.provide(OpenAi, NodeHttpClient.layerUndici)
```

## Running the Program

Now that we have a `Layer` which provides us with an `OpenAiClient`, we're ready to make our `main` program runnable. 

Our final program looks like the following:

```ts twoslash
import { OpenAiClient, OpenAiCompletions } from "@effect/ai-openai"
import { Completions } from "@effect/ai"
import { NodeHttpClient } from "@effect/platform-node"
import { Config, Console, Effect, Layer } from "effect"

const generateDadJoke = Effect.gen(function*() {
  const completions = yield* Completions.Completions
  return yield* completions.create("Generate a hilarious dad joke").pipe(
    Effect.andThen((response) => Console.log(response.text))
  )
})

const Gpt4o = OpenAiCompletions.model("gpt-4o")

const main = Effect.gen(function*() {
  const gpt4o = yield* Gpt4o
  yield* gpt4o.provide(generateDadJoke)
})

const OpenAi = OpenAiClient.layerConfig({
  apiKey: Config.redacted("OPENAI_API_KEY")
})

const OpenAiWithHttp = Layer.provide(OpenAi, NodeHttpClient.layerUndici)

main.pipe(
  Effect.provide(OpenAiWithHttp),
  Effect.runPromise
)
```
